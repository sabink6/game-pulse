{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_reviews_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_reviews_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sabina\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sabina\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Sabina\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sabina\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Sabina\\anaconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_reviews_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')  # Ensure full tokenizer support\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add domain-specific words to the stop words list\n",
    "stop_words.update([\"steam\"])\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"sample_reviews_dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing: Splitting reviews into sentences & cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    recommendationid                                     clean_sentence  \\\n",
      "0           70427607  game elements many games sewn one incredibly well   \n",
      "1           70427607  bit survival fps space sim trading farming bas...   \n",
      "2           70427607  result beautifully presented journey discovery...   \n",
      "3           70427607               would recommend everyone adventurous   \n",
      "4           70426209                            game random gen presets   \n",
      "5           70426209                                       voice acting   \n",
      "6           70426209                                           gets old   \n",
      "7           70426209                               none less swell time   \n",
      "8           70425814                     first played years ago fun bit   \n",
      "9           70425814                    started playing recently better   \n",
      "10          70425814  criticisms many asteroids asteroids spaced bet...   \n",
      "11          70425814                        deeper space less asteroids   \n",
      "12          70425814  cluttered least immersive part feels like lot ...   \n",
      "13          70425814  species characters freighter might change sudd...   \n",
      "14          70425814                           weird stuff gives dejavu   \n",
      "15          70425814                                cest tres confusing   \n",
      "16          70425814  idea materials gon important theres enough roo...   \n",
      "17          70425814  travel one system another quest quest requires...   \n",
      "18          70425814  related issue planet saying material find sing...   \n",
      "19          70425814  abandoned crashed sunk freighters hard navigat...   \n",
      "\n",
      "                                   tokenized_sentence  voted_up language  \\\n",
      "0   this game has the elements of many games sewn ...         1  english   \n",
      "1   a bit of survival fps space sim trading farmin...         1  english   \n",
      "2   the result is a beautifully presented journey ...         1  english   \n",
      "3      would recommend to everyone who is adventurous         1  english   \n",
      "4                   game is k random gen from presets         1  english   \n",
      "5                                     no voice acting         1  english   \n",
      "6                                            gets old         1  english   \n",
      "7                  but none the less was a swell time         1  english   \n",
      "8   i first played 2 years ago and it was fun for ...         1  english   \n",
      "9   then i started playing again recently and its ...         1  english   \n",
      "10  criticisms too many asteroids asteroids should...         1  english   \n",
      "11  the deeper into space you go the less asteroid...         1  english   \n",
      "12  its just so cluttered its the least immersive ...         1  english   \n",
      "13  species of characters on your freighter might ...         1  english   \n",
      "14             just weird stuff that gives you dejavu         1  english   \n",
      "15                                cest tres confusing         1  english   \n",
      "16  i have no idea what materials are gon na be im...         1  english   \n",
      "17  when i travel from one system to another to do...         1  english   \n",
      "18  which is related to the issue of a planet sayi...         1  english   \n",
      "19  abandoned crashed sunk freighters are hard to ...         1  english   \n",
      "\n",
      "      timestamp_created    timestamp_updated  votes_up  votes_funny  \\\n",
      "0   2020-06-07 07:33:21  2020-06-07 07:33:21         0            0   \n",
      "1   2020-06-07 07:33:21  2020-06-07 07:33:21         0            0   \n",
      "2   2020-06-07 07:33:21  2020-06-07 07:33:21         0            0   \n",
      "3   2020-06-07 07:33:21  2020-06-07 07:33:21         0            0   \n",
      "4   2020-06-07 06:48:46  2020-06-07 06:48:46         0            0   \n",
      "5   2020-06-07 06:48:46  2020-06-07 06:48:46         0            0   \n",
      "6   2020-06-07 06:48:46  2020-06-07 06:48:46         0            0   \n",
      "7   2020-06-07 06:48:46  2020-06-07 06:48:46         0            0   \n",
      "8   2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "9   2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "10  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "11  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "12  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "13  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "14  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "15  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "16  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "17  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "18  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "19  2020-06-07 06:35:34  2020-06-07 06:35:34         0            0   \n",
      "\n",
      "    weighted_vote_score  comment_count  steam_purchase  received_for_free  \\\n",
      "0                   0.0              0           False              False   \n",
      "1                   0.0              0           False              False   \n",
      "2                   0.0              0           False              False   \n",
      "3                   0.0              0           False              False   \n",
      "4                   0.0              0           False              False   \n",
      "5                   0.0              0           False              False   \n",
      "6                   0.0              0           False              False   \n",
      "7                   0.0              0           False              False   \n",
      "8                   0.0              0            True              False   \n",
      "9                   0.0              0            True              False   \n",
      "10                  0.0              0            True              False   \n",
      "11                  0.0              0            True              False   \n",
      "12                  0.0              0            True              False   \n",
      "13                  0.0              0            True              False   \n",
      "14                  0.0              0            True              False   \n",
      "15                  0.0              0            True              False   \n",
      "16                  0.0              0            True              False   \n",
      "17                  0.0              0            True              False   \n",
      "18                  0.0              0            True              False   \n",
      "19                  0.0              0            True              False   \n",
      "\n",
      "    written_during_early_access  author_num_games_owned  author_num_reviews  \\\n",
      "0                         False                     143                   1   \n",
      "1                         False                     143                   1   \n",
      "2                         False                     143                   1   \n",
      "3                         False                     143                   1   \n",
      "4                         False                     133                   5   \n",
      "5                         False                     133                   5   \n",
      "6                         False                     133                   5   \n",
      "7                         False                     133                   5   \n",
      "8                         False                     670                  31   \n",
      "9                         False                     670                  31   \n",
      "10                        False                     670                  31   \n",
      "11                        False                     670                  31   \n",
      "12                        False                     670                  31   \n",
      "13                        False                     670                  31   \n",
      "14                        False                     670                  31   \n",
      "15                        False                     670                  31   \n",
      "16                        False                     670                  31   \n",
      "17                        False                     670                  31   \n",
      "18                        False                     670                  31   \n",
      "19                        False                     670                  31   \n",
      "\n",
      "    author_playtime_forever  author_playtime_last_two_weeks  \\\n",
      "0                     14368                            1041   \n",
      "1                     14368                            1041   \n",
      "2                     14368                            1041   \n",
      "3                     14368                            1041   \n",
      "4                      3927                              51   \n",
      "5                      3927                              51   \n",
      "6                      3927                              51   \n",
      "7                      3927                              51   \n",
      "8                      1942                             987   \n",
      "9                      1942                             987   \n",
      "10                     1942                             987   \n",
      "11                     1942                             987   \n",
      "12                     1942                             987   \n",
      "13                     1942                             987   \n",
      "14                     1942                             987   \n",
      "15                     1942                             987   \n",
      "16                     1942                             987   \n",
      "17                     1942                             987   \n",
      "18                     1942                             987   \n",
      "19                     1942                             987   \n",
      "\n",
      "     author_last_played  \n",
      "0   2020-06-02 07:05:32  \n",
      "1   2020-06-02 07:05:32  \n",
      "2   2020-06-02 07:05:32  \n",
      "3   2020-06-02 07:05:32  \n",
      "4   2020-06-02 11:31:12  \n",
      "5   2020-06-02 11:31:12  \n",
      "6   2020-06-02 11:31:12  \n",
      "7   2020-06-02 11:31:12  \n",
      "8   2020-06-07 06:21:38  \n",
      "9   2020-06-07 06:21:38  \n",
      "10  2020-06-07 06:21:38  \n",
      "11  2020-06-07 06:21:38  \n",
      "12  2020-06-07 06:21:38  \n",
      "13  2020-06-07 06:21:38  \n",
      "14  2020-06-07 06:21:38  \n",
      "15  2020-06-07 06:21:38  \n",
      "16  2020-06-07 06:21:38  \n",
      "17  2020-06-07 06:21:38  \n",
      "18  2020-06-07 06:21:38  \n",
      "19  2020-06-07 06:21:38  \n"
     ]
    }
   ],
   "source": [
    "# Check if review column exists\n",
    "if 'review' not in df.columns:\n",
    "    raise ValueError(\"The dataset does not contain a 'review' column\")\n",
    "\n",
    "# Remove rows with empty review strings\n",
    "df = df.dropna(subset=['review'])\n",
    "\n",
    "# Select relevant columns\n",
    "#df = df[['recommendationid', 'review', 'voted_up']].dropna()\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert sentiment labels (True -> 1, False -> 0)\n",
    "df['label'] = df['voted_up'].astype(int)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if len(word) > 2 and word not in stop_words]  # Filter stopwords & short words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to clean text\n",
    "def tokenize_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to split reviews into unique sentences\n",
    "def split_into_sentences(row):\n",
    "    #sentences = list(set(sent_tokenize(row['review'])))  # Ensure unique sentences per review\n",
    "    sentences = list(sent_tokenize(row['review']))\n",
    "    cleaned_sentences = [clean_text(sentence) for sentence in sentences]\n",
    "    token_sentences = [tokenize_text(sentence) for sentence in sentences]\n",
    "    return [{'recommendationid': row['recommendationid'],\n",
    "             'clean_sentence': sentence,\n",
    "             'tokenized_sentence': to_sentence,\n",
    "             'voted_up': row['label'],\n",
    "            'language' : row['language'],\n",
    "            'timestamp_created' : row['timestamp_created'],\n",
    "            'timestamp_updated' : row['timestamp_updated'],\n",
    "            'votes_up' : row['votes_up'],\n",
    "            'votes_funny' : row['votes_funny'],\n",
    "            'weighted_vote_score' : row['weighted_vote_score'],\n",
    "            'comment_count' : row['comment_count'],\n",
    "            'steam_purchase' : row['steam_purchase'],\n",
    "            'received_for_free' : row['received_for_free'],\n",
    "            'written_during_early_access' : row['written_during_early_access'],\n",
    "            'author_num_games_owned' : row['author_num_games_owned'],\n",
    "            'author_num_reviews' : row['author_num_reviews'],\n",
    "            'author_playtime_forever' : row['author_playtime_forever'],\n",
    "            'author_playtime_last_two_weeks' : row['author_playtime_last_two_weeks'],\n",
    "            'author_last_played' : row['author_last_played']\n",
    "            } for sentence, to_sentence in zip(cleaned_sentences, token_sentences)]\n",
    "\n",
    "\n",
    "# Expand reviews into individual unique sentences\n",
    "sentence_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence_data.extend(split_into_sentences(row))\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_data).drop_duplicates()\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.describe\n",
    "df_sentences.to_csv('./alt_no_mans_sky_steam_review_data_split_reviews_no_index_FULL.csv', index=False) #Save the dataframe with all the extracted sentences and extra columns \n",
    "#df_sentences.to_csv('./split_reviews_no_index.csv', index=False)\n",
    "#df_sentences.to_csv('./split_reviews_with_index.csv', index=True)\n",
    "df_sentences = df_sentences[['recommendationid', 'clean_sentence', 'tokenized_sentence', 'voted_up']]\n",
    "df_sentences.to_csv('./alt_no_mans_sky_steam_review_data_split_reviews_no_index_reduced_columns.csv', index=False) #Save the dataframe with just the extracted sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "word_freq = {}\n",
    "for sentence in df_sentences['sentence']:\n",
    "    for word in sentence.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    if len(word) > 2 and freq >= 5:  # Exclude short and rare words\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert text to indices\n",
    "def text_to_indices(text):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "\n",
    "df_sentences['indexed_sentence'] = df_sentences['sentence'].apply(text_to_indices)\n",
    "\n",
    "# Dynamically determine max sequence length\n",
    "max_len = int(df_sentences['indexed_sentence'].apply(len).quantile(0.95))\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len=max_len):\n",
    "    return seq[:max_len] + [vocab['<PAD>']] * max(0, max_len - len(seq))\n",
    "\n",
    "df_sentences['padded_sentence'] = df_sentences['indexed_sentence'].apply(lambda x: pad_sequence(x, max_len=max_len))\n",
    "\n",
    "# Split into train and test sets\n",
    "df_train, df_test = train_test_split(df_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrame to PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.sentences = torch.tensor(data['padded_sentence'].tolist(), dtype=torch.long)\n",
    "        self.labels = torch.tensor(data['label'].tolist(), dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# Initialize Dataloaders\n",
    "batch_size = 128\n",
    "dataset_train = SentimentDataset(df_train)\n",
    "dataset_test = SentimentDataset(df_test)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU/CPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6487\n",
      "Epoch 2/5, Loss: 0.4859\n",
      "Epoch 3/5, Loss: 0.4261\n",
      "Epoch 4/5, Loss: 0.4340\n",
      "Epoch 5/5, Loss: 0.4190\n",
      "   recommendationid                                           sentence  label  \\\n",
      "0          70427607  result beautifully presented journey discovery...      1   \n",
      "1          70427607               would recommend everyone adventurous      1   \n",
      "2          70427607  game elements many games sewn one incredibly well      1   \n",
      "3          70427607  bit survival fps space sim trading farming bas...      1   \n",
      "4          70426209                                       voice acting      1   \n",
      "\n",
      "                                    indexed_sentence  \\\n",
      "0                  [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]   \n",
      "1                                       [3, 4, 5, 1]   \n",
      "2                          [6, 1, 7, 8, 1, 9, 1, 10]   \n",
      "3  [11, 12, 1, 13, 1, 14, 1, 15, 16, 1, 1, 1, 17,...   \n",
      "4                                             [1, 1]   \n",
      "\n",
      "                                     padded_sentence  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, ...  \n",
      "1  [3, 4, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [6, 1, 7, 8, 1, 9, 1, 10, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "3  [11, 12, 1, 13, 1, 14, 1, 15, 16, 1, 1, 1, 17,...  \n",
      "4  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=1, n_layers=2, drop_prob=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out.mean(dim=1))  # Mean pooling instead of last output\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SentimentLSTM(vocab_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # More numerically stable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader_train, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4111, Accuracy: 0.8560\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) >= 0.5).long()\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f'Test Loss: {total_loss/len(dataloader):.4f}, Accuracy: {correct/total:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dataloader_test, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8464\n",
      "    recommendationid                                           sentence  \\\n",
      "0           70427607  result beautifully presented journey discovery...   \n",
      "1           70427607               would recommend everyone adventurous   \n",
      "2           70427607  game elements many games sewn one incredibly well   \n",
      "3           70427607  bit survival fps space sim trading farming bas...   \n",
      "4           70426209                                       voice acting   \n",
      "5           70426209                            game random gen presets   \n",
      "6           70426209                               none less swell time   \n",
      "7           70426209                                           gets old   \n",
      "8           70425814  criticisms many asteroids asteroids spaced bet...   \n",
      "9           70425814                                      civilisations   \n",
      "10          70425814                           waste time trying figure   \n",
      "11          70425814                           weird stuff gives dejavu   \n",
      "12          70425814  species characters freighter might change sudd...   \n",
      "13          70425814                     many ways customise experience   \n",
      "14          70425814  cluttered least immersive part feels like lot ...   \n",
      "15          70425814                       found stuff one havent since   \n",
      "16          70425814                     first played years ago fun bit   \n",
      "17          70425814  travel one system another quest quest requires...   \n",
      "18          70425814  theres lot tedious stuff takes away otherwise ...   \n",
      "19          70425814                    started playing recently better   \n",
      "\n",
      "    label  predicted_lstm_score  predicted_lstm_label  \n",
      "0       1              0.781591                     1  \n",
      "1       1              0.882141                     1  \n",
      "2       1              0.828298                     1  \n",
      "3       1              0.708814                     1  \n",
      "4       1              0.911105                     1  \n",
      "5       1              0.887222                     1  \n",
      "6       1              0.885210                     1  \n",
      "7       1              0.911105                     1  \n",
      "8       1              0.811873                     1  \n",
      "9       1              0.921319                     1  \n",
      "10      1              0.886998                     1  \n",
      "11      1              0.888094                     1  \n",
      "12      1              0.759236                     1  \n",
      "13      1              0.887704                     1  \n",
      "14      1              0.719153                     1  \n",
      "15      1              0.869227                     1  \n",
      "16      1              0.852195                     1  \n",
      "17      1              0.594433                     1  \n",
      "18      1              0.804771                     1  \n",
      "19      1              0.887208                     1  \n"
     ]
    }
   ],
   "source": [
    "# Initialize Dataloaders\n",
    "batch_size = 128\n",
    "dataset_sentences = SentimentDataset(df_sentences)\n",
    "dataloader_sentences = DataLoader(dataset_sentences, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Predict sentiment scores\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predictions.extend(torch.sigmoid(outputs).tolist())\n",
    "    return predictions\n",
    "\n",
    "# Get predictions for entire dataset\n",
    "df_sentences['predicted_lstm_score'] = predict_sentiment(model, dataloader_sentences)\n",
    "df_sentences['predicted_lstm_label'] = (df_sentences['predicted_lstm_score'] >= 0.5).astype(int)\n",
    "\n",
    "# Append predictions next to label column\n",
    "df_sentences = df_sentences[['recommendationid', 'sentence', 'label', 'predicted_lstm_score', 'predicted_lstm_label']]\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (df_sentences['predicted_lstm_label'] == df_sentences['label']).mean()\n",
    "print(f'Overall Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the updated dataset\n",
    "#df_sentences.to_csv(\"predicted_sentiment_dataset.csv\", index=False)\n",
    "\n",
    "print(df_sentences.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
