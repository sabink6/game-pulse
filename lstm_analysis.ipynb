{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')  # Ensure full tokenizer support\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add domain-specific words to the stop words list\n",
    "stop_words.update([\"steam\"])\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"sample_reviews_dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing: Splitting reviews into sentences & cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    recommendationid                                           sentence  label\n",
      "0           70427607  result beautifully presented journey discovery...      1\n",
      "1           70427607               would recommend everyone adventurous      1\n",
      "2           70427607  game elements many games sewn one incredibly well      1\n",
      "3           70427607  bit survival fps space sim trading farming bas...      1\n",
      "4           70426209                                       voice acting      1\n",
      "5           70426209                            game random gen presets      1\n",
      "6           70426209                               none less swell time      1\n",
      "7           70426209                                           gets old      1\n",
      "8           70425814  criticisms many asteroids asteroids spaced bet...      1\n",
      "9           70425814                                      civilisations      1\n",
      "10          70425814                           waste time trying figure      1\n",
      "11          70425814                           weird stuff gives dejavu      1\n",
      "12          70425814  species characters freighter might change sudd...      1\n",
      "13          70425814                     many ways customise experience      1\n",
      "14          70425814  cluttered least immersive part feels like lot ...      1\n",
      "15          70425814                       found stuff one havent since      1\n",
      "16          70425814                     first played years ago fun bit      1\n",
      "17          70425814  travel one system another quest quest requires...      1\n",
      "18          70425814  theres lot tedious stuff takes away otherwise ...      1\n",
      "19          70425814                    started playing recently better      1\n"
     ]
    }
   ],
   "source": [
    "# Check if review column exists\n",
    "if 'review' not in df.columns:\n",
    "    raise ValueError(\"The dataset does not contain a 'review' column\")\n",
    "\n",
    "# Remove rows with empty review strings\n",
    "df = df.dropna(subset=['review'])\n",
    "\n",
    "# Select relevant columns\n",
    "df = df[['recommendationid', 'review', 'voted_up']].dropna()\n",
    "\n",
    "# Convert sentiment labels (True -> 1, False -> 0)\n",
    "df['label'] = df['voted_up'].astype(int)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if len(word) > 2 and word not in stop_words]  # Filter stopwords & short words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to split reviews into unique sentences\n",
    "def split_into_sentences(row):\n",
    "    sentences = list(set(sent_tokenize(row['review'])))  # Ensure unique sentences per review\n",
    "    cleaned_sentences = [clean_text(sentence) for sentence in sentences]\n",
    "    return [{'recommendationid': row['recommendationid'], 'sentence': sentence, 'label': row['label']} for sentence in cleaned_sentences]\n",
    "\n",
    "# Expand reviews into individual unique sentences\n",
    "sentence_data = []\n",
    "for _, row in df.iterrows():\n",
    "    sentence_data.extend(split_into_sentences(row))\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_data).drop_duplicates()\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "word_freq = {}\n",
    "for sentence in df_sentences['sentence']:\n",
    "    for word in sentence.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    if len(word) > 2 and freq >= 5:  # Exclude short and rare words\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert text to indices\n",
    "def text_to_indices(text):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "\n",
    "df_sentences['indexed_sentence'] = df_sentences['sentence'].apply(text_to_indices)\n",
    "\n",
    "# Dynamically determine max sequence length\n",
    "max_len = int(df_sentences['indexed_sentence'].apply(len).quantile(0.95))\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len=max_len):\n",
    "    return seq[:max_len] + [vocab['<PAD>']] * max(0, max_len - len(seq))\n",
    "\n",
    "df_sentences['padded_sentence'] = df_sentences['indexed_sentence'].apply(lambda x: pad_sequence(x, max_len=max_len))\n",
    "\n",
    "# Split into train and test sets\n",
    "df_train, df_test = train_test_split(df_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrame to PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.sentences = torch.tensor(data['padded_sentence'].tolist(), dtype=torch.long)\n",
    "        self.labels = torch.tensor(data['label'].tolist(), dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# Initialize Dataloaders\n",
    "batch_size = 128\n",
    "dataset_train = SentimentDataset(df_train)\n",
    "dataset_test = SentimentDataset(df_test)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU/CPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6487\n",
      "Epoch 2/5, Loss: 0.4859\n",
      "Epoch 3/5, Loss: 0.4261\n",
      "Epoch 4/5, Loss: 0.4340\n",
      "Epoch 5/5, Loss: 0.4190\n",
      "   recommendationid                                           sentence  label  \\\n",
      "0          70427607  result beautifully presented journey discovery...      1   \n",
      "1          70427607               would recommend everyone adventurous      1   \n",
      "2          70427607  game elements many games sewn one incredibly well      1   \n",
      "3          70427607  bit survival fps space sim trading farming bas...      1   \n",
      "4          70426209                                       voice acting      1   \n",
      "\n",
      "                                    indexed_sentence  \\\n",
      "0                  [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]   \n",
      "1                                       [3, 4, 5, 1]   \n",
      "2                          [6, 1, 7, 8, 1, 9, 1, 10]   \n",
      "3  [11, 12, 1, 13, 1, 14, 1, 15, 16, 1, 1, 1, 17,...   \n",
      "4                                             [1, 1]   \n",
      "\n",
      "                                     padded_sentence  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, ...  \n",
      "1  [3, 4, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [6, 1, 7, 8, 1, 9, 1, 10, 0, 0, 0, 0, 0, 0, 0,...  \n",
      "3  [11, 12, 1, 13, 1, 14, 1, 15, 16, 1, 1, 1, 17,...  \n",
      "4  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=1, n_layers=2, drop_prob=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out.mean(dim=1))  # Mean pooling instead of last output\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SentimentLSTM(vocab_size).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # More numerically stable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader_train, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4111, Accuracy: 0.8560\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) >= 0.5).long()\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f'Test Loss: {total_loss/len(dataloader):.4f}, Accuracy: {correct/total:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dataloader_test, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8464\n",
      "    recommendationid                                           sentence  \\\n",
      "0           70427607  result beautifully presented journey discovery...   \n",
      "1           70427607               would recommend everyone adventurous   \n",
      "2           70427607  game elements many games sewn one incredibly well   \n",
      "3           70427607  bit survival fps space sim trading farming bas...   \n",
      "4           70426209                                       voice acting   \n",
      "5           70426209                            game random gen presets   \n",
      "6           70426209                               none less swell time   \n",
      "7           70426209                                           gets old   \n",
      "8           70425814  criticisms many asteroids asteroids spaced bet...   \n",
      "9           70425814                                      civilisations   \n",
      "10          70425814                           waste time trying figure   \n",
      "11          70425814                           weird stuff gives dejavu   \n",
      "12          70425814  species characters freighter might change sudd...   \n",
      "13          70425814                     many ways customise experience   \n",
      "14          70425814  cluttered least immersive part feels like lot ...   \n",
      "15          70425814                       found stuff one havent since   \n",
      "16          70425814                     first played years ago fun bit   \n",
      "17          70425814  travel one system another quest quest requires...   \n",
      "18          70425814  theres lot tedious stuff takes away otherwise ...   \n",
      "19          70425814                    started playing recently better   \n",
      "\n",
      "    label  predicted_lstm_score  predicted_lstm_label  \n",
      "0       1              0.781591                     1  \n",
      "1       1              0.882141                     1  \n",
      "2       1              0.828298                     1  \n",
      "3       1              0.708814                     1  \n",
      "4       1              0.911105                     1  \n",
      "5       1              0.887222                     1  \n",
      "6       1              0.885210                     1  \n",
      "7       1              0.911105                     1  \n",
      "8       1              0.811873                     1  \n",
      "9       1              0.921319                     1  \n",
      "10      1              0.886998                     1  \n",
      "11      1              0.888094                     1  \n",
      "12      1              0.759236                     1  \n",
      "13      1              0.887704                     1  \n",
      "14      1              0.719153                     1  \n",
      "15      1              0.869227                     1  \n",
      "16      1              0.852195                     1  \n",
      "17      1              0.594433                     1  \n",
      "18      1              0.804771                     1  \n",
      "19      1              0.887208                     1  \n"
     ]
    }
   ],
   "source": [
    "# Initialize Dataloaders\n",
    "batch_size = 128\n",
    "dataset_sentences = SentimentDataset(df_sentences)\n",
    "dataloader_sentences = DataLoader(dataset_sentences, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Predict sentiment scores\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predictions.extend(torch.sigmoid(outputs).tolist())\n",
    "    return predictions\n",
    "\n",
    "# Get predictions for entire dataset\n",
    "df_sentences['predicted_lstm_score'] = predict_sentiment(model, dataloader_sentences)\n",
    "df_sentences['predicted_lstm_label'] = (df_sentences['predicted_lstm_score'] >= 0.5).astype(int)\n",
    "\n",
    "# Append predictions next to label column\n",
    "df_sentences = df_sentences[['recommendationid', 'sentence', 'label', 'predicted_lstm_score', 'predicted_lstm_label']]\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (df_sentences['predicted_lstm_label'] == df_sentences['label']).mean()\n",
    "print(f'Overall Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the updated dataset\n",
    "#df_sentences.to_csv(\"predicted_sentiment_dataset.csv\", index=False)\n",
    "\n",
    "print(df_sentences.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
