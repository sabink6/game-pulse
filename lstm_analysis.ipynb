{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sabina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"sample reviews dataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing: Splitting reviews into sentences & cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   recommendationid                                           sentence  label\n",
      "0          70427607  game elements many games sewn one incredibly well      1\n",
      "1          70427607  bit survival fps space sim trading farming bas...      1\n",
      "2          70427607  result beautifully presented journey discovery...      1\n",
      "3          70427607               would recommend everyone adventurous      1\n",
      "4          70426209                          game k random gen presets      1\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with empty review strings\n",
    "df = df.dropna(subset=['review'])\n",
    "\n",
    "# Select relevant columns\n",
    "df_individual = df[['recommendationid', 'review', 'voted_up']].dropna()\n",
    "\n",
    "# Convert sentiment labels (True -> 1, False -> 0)\n",
    "df_individual['label'] = df_individual['voted_up'].astype(int)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to split reviews into sentences\n",
    "def split_into_sentences(row):\n",
    "    sentences = sent_tokenize(row['review'])\n",
    "    cleaned_sentences = [clean_text(sentence) for sentence in sentences]\n",
    "    return [{'recommendationid': row['recommendationid'], 'sentence': sentence, 'label': row['label']} for sentence in cleaned_sentences]\n",
    "\n",
    "# Expand reviews into individual sentences\n",
    "sentence_data = []\n",
    "for _, row in df_individual.iterrows():\n",
    "    sentence_data.extend(split_into_sentences(row))\n",
    "\n",
    "# Create a new DataFrame with split sentences\n",
    "df_sentences = pd.DataFrame(sentence_data)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary creation for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "word_freq = {}\n",
    "for sentence in df_sentences['sentence']:\n",
    "    for word in sentence.split():\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "for word, freq in word_freq.items():\n",
    "    if freq >= 5:  # Threshold for rare words\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert text to indices\n",
    "def text_to_indices(text):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "\n",
    "df_sentences['indexed_sentence'] = df_sentences['sentence'].apply(text_to_indices)\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len=50):\n",
    "    return seq[:max_len] + [vocab['<PAD>']] * max(0, max_len - len(seq))\n",
    "\n",
    "df_sentences['padded_sentence'] = df_sentences['indexed_sentence'].apply(lambda x: pad_sequence(x, max_len=50))\n",
    "\n",
    "# Split into train and test sets\n",
    "df_train, df_test = train_test_split(df_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrame to PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.sentences = torch.tensor(data['padded_sentence'].tolist(), dtype=torch.long)\n",
    "        self.labels = torch.tensor(data['label'].tolist(), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# Create train and test DataLoaders\n",
    "batch_size = 128\n",
    "dataset_train = SentimentDataset(df_train)\n",
    "dataset_test = SentimentDataset(df_test)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5905\n",
      "Epoch 2/5, Loss: 0.4376\n",
      "Epoch 3/5, Loss: 0.4396\n",
      "Epoch 4/5, Loss: 0.4281\n",
      "Epoch 5/5, Loss: 0.4299\n",
      "   recommendationid                                           sentence  label  \\\n",
      "0          70427607  game elements many games sewn one incredibly well      1   \n",
      "1          70427607  bit survival fps space sim trading farming bas...      1   \n",
      "2          70427607  result beautifully presented journey discovery...      1   \n",
      "3          70427607               would recommend everyone adventurous      1   \n",
      "4          70426209                          game k random gen presets      1   \n",
      "\n",
      "                                  indexed_sentence  \\\n",
      "0                         [2, 1, 3, 4, 1, 5, 1, 6]   \n",
      "1  [7, 8, 1, 9, 1, 10, 1, 11, 12, 1, 1, 1, 13, 14]   \n",
      "2               [1, 1, 1, 1, 1, 1, 1, 15, 1, 1, 1]   \n",
      "3                                  [16, 17, 18, 1]   \n",
      "4                                  [2, 1, 1, 1, 1]   \n",
      "\n",
      "                                     padded_sentence  \n",
      "0  [2, 1, 3, 4, 1, 5, 1, 6, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [7, 8, 1, 9, 1, 10, 1, 11, 12, 1, 1, 1, 13, 14...  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 15, 1, 1, 1, 0, 0, 0, 0,...  \n",
      "3  [16, 17, 18, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define LSTM Model\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=1, n_layers=2, drop_prob=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SentimentLSTM(vocab_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_sentences.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4250, Accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            predicted = (outputs >= 0.5).long()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f'Test Loss: {total_loss/len(dataloader):.4f}, Accuracy: {correct/total:.4f}')\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dataloader_test, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
