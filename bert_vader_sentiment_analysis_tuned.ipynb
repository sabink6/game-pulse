import pandas as pd
import torch
import re
import nltk
import matplotlib.pyplot as plt
import numpy as np
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from collections import Counter
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# Ensure necessary NLTK resources are available
def download_nltk_resources():
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('vader_lexicon')

download_nltk_resources()

# Load dataset
file_path = "Steam review data from previous study(in).csv"
df = pd.read_csv(file_path, encoding='utf-8')

if 'review' not in df.columns:
    raise ValueError("The dataset does not contain a 'review' column")

df = df[['recommendationid', 'review']].dropna().drop_duplicates()

# Define stopwords and negations
stop_words = set(stopwords.words('english'))
negations = {"not", "never", "none", "no", "don't", "haven't", "isn't"}
stop_words = stop_words - negations  # Keep negations

# Text preprocessing
def clean_text(text, remove_stopwords=True):
    text = re.sub(r"[^a-zA-Z0-9\sâ€™']", "", text.lower().strip())
    words = word_tokenize(text)
    if remove_stopwords:
        words = [word for word in words if word not in stop_words or word in negations]
    return " ".join(words)

# Process reviews
def split_into_sentences(row):
    sentences = [s for s in sent_tokenize(str(row['review'])) if re.search(r"[a-zA-Z0-9]", s)]
    return [{"recommendationid": row["recommendationid"],
             "sentence_with_stopwords": clean_text(s, False),
             "sentence_without_stopwords": clean_text(s)} for s in sentences]

df_sentences = pd.DataFrame([entry for _, row in df.iterrows() for entry in split_into_sentences(row)]).drop_duplicates()

# Sentiment analysis setup
sia = SentimentIntensityAnalyzer()

# Load BERT model for sentiment analysis
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)

def get_vader_sentiment(text):
    scores = sia.polarity_scores(text)
    return 1 if scores['compound'] >= 0 else 0

def get_bert_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    outputs = model(**inputs)
    scores = outputs.logits.softmax(dim=1).cpu().detach().numpy()[0]
    return 1 if np.argmax(scores) > 2 else 0

df_sentences['vader_sentiment'] = df_sentences['sentence_with_stopwords'].apply(get_vader_sentiment)
df_sentences['bert_sentiment'] = df_sentences['sentence_with_stopwords'].apply(get_bert_sentiment)
df_sentences['combined_sentiment'] = (df_sentences['vader_sentiment'] + df_sentences['bert_sentiment']) / 2

def final_sentiment(value):
    return 1 if value > 0.5 else 0

df_sentences['final_sentiment'] = df_sentences['combined_sentiment'].apply(final_sentiment)

# Evaluation
true_labels = df_sentences['vader_sentiment'].tolist()
pred_labels = df_sentences['final_sentiment'].tolist()

print("Model Accuracy:", accuracy_score(true_labels, pred_labels))
print("Classification Report:\n", classification_report(true_labels, pred_labels))

# Confusion Matrix
cm = confusion_matrix(true_labels, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
